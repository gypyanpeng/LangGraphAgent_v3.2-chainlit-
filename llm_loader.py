# llm_loader.py
import json
import os
from typing import Optional
from langchain_openai import ChatOpenAI

# å¯¼å…¥è‡ªå®šä¹‰ Ollama é€‚é…å™¨
try:
    from ollama_adapter import create_ollama_chat_model
    OLLAMA_ADAPTER_AVAILABLE = True
except ImportError:
    OLLAMA_ADAPTER_AVAILABLE = False

def load_llm_from_config(config_path: str = "config/llm_config.json", provider: Optional[str] = None):
    """
    åŠ è½½ LLM æ¨¡å‹ï¼Œæ”¯æŒå¤šç§æä¾›å•†åŒ…æ‹¬ Ollama è‡ªå®šä¹‰é€‚é…å™¨ã€‚
    æ”¯æŒå¤šæ¨¡å‹é…ç½®ï¼Œprovider å¯é€‰ï¼Œé»˜è®¤åŠ è½½ default_providerã€‚
    é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„ä¸º config/llm_config.jsonã€‚
    """
    with open(config_path, "r", encoding="utf-8") as f:
        config = json.load(f)
    # å…¼å®¹å•æ¨¡å‹å’Œå¤šæ¨¡å‹é…ç½®
    if "models" in config:
        models = config["models"]
        default_provider = config.get("default_provider")
        provider = provider or default_provider
        if provider not in models:
            raise ValueError(f"æœªæ‰¾åˆ° provider: {provider} çš„æ¨¡å‹é…ç½®")
        llm_conf = models[provider]
        model = llm_conf.get("model_name")
        api_key = llm_conf.get("api_key")
        api_base = llm_conf.get("base_url")
        temperature = llm_conf.get("temperature", 0.7)
        streaming = llm_conf.get("streaming", True)
        provider_type = llm_conf.get("provider", provider)
    else:
        llm_conf = config["llm"]
        model = llm_conf.get("model")
        api_key = llm_conf.get("api_key")
        api_base = llm_conf.get("api_base")
        temperature = llm_conf.get("temperature", 0.7)
        streaming = llm_conf.get("streaming", True)
        provider_type = llm_conf.get("provider", "openai")

    # å¦‚æœæ˜¯ Ollama æä¾›å•†ä¸”è‡ªå®šä¹‰é€‚é…å™¨å¯ç”¨ï¼Œä½¿ç”¨è‡ªå®šä¹‰é€‚é…å™¨
    if provider_type == "ollama" and OLLAMA_ADAPTER_AVAILABLE:
        print(f"ğŸ”§ ä½¿ç”¨ Ollama è‡ªå®šä¹‰é€‚é…å™¨: {model}")
        return create_ollama_chat_model(
            model=model,
            base_url=api_base,
            temperature=temperature,
            streaming=streaming
        )
    else:
        # ä½¿ç”¨æ ‡å‡† OpenAI å…¼å®¹æ¥å£
        if provider_type == "ollama" and not OLLAMA_ADAPTER_AVAILABLE:
            print("âš ï¸ Ollama é€‚é…å™¨ä¸å¯ç”¨ï¼Œå°è¯•ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£")

        return ChatOpenAI(
            model=model,
            api_key=api_key,
            base_url=api_base,
            temperature=temperature,
            streaming=streaming
        )

def list_available_models(config_path: str = "llm_config.json"):
    """
    åˆ—å‡ºé…ç½®æ–‡ä»¶ä¸­æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹ã€‚

    Args:
        config_path (str): LLMé…ç½®æ–‡ä»¶çš„è·¯å¾„ã€‚

    Returns:
        dict: åŒ…å«æ‰€æœ‰æ¨¡å‹é…ç½®çš„å­—å…¸ã€‚
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ '{config_path}' æœªæ‰¾åˆ°ã€‚")
        return {}
    except json.JSONDecodeError:
        print(f"é”™è¯¯: æ— æ³•è§£æé…ç½®æ–‡ä»¶ '{config_path}'ã€‚")
        return {}

    if "models" not in config:
        print("âš ï¸ é…ç½®æ–‡ä»¶ä½¿ç”¨æ—§æ ¼å¼ï¼Œä¸æ”¯æŒå¤šæ¨¡å‹åˆ—è¡¨")
        return {}

    models = config.get("models", {})
    default_provider = config.get("default_provider")

    print("ğŸ“‹ å¯ç”¨æ¨¡å‹åˆ—è¡¨:")
    for provider, model_config in models.items():
        default_mark = " (é»˜è®¤)" if provider == default_provider else ""
        description = model_config.get("description", "")

        print(f"  - {provider}{default_mark}")
        print(f"    æ¨¡å‹: {model_config.get('model_name', 'N/A')}")
        print(f"    æè¿°: {description}")
        print(f"    URL: {model_config.get('base_url', 'N/A')}")
        print()

    return models