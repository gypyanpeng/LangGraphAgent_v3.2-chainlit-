"""
Ollama è‡ªå®šä¹‰é€‚é…å™¨
è§£å†³ Ollama OpenAI å…¼å®¹æ¥å£çš„ 502 é”™è¯¯é—®é¢˜
"""

import json
import requests
from typing import List, Dict, Any, Optional, AsyncIterator
from langchain_core.language_models.llms import LLM
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, AIMessage, HumanMessage
from langchain_core.outputs import ChatResult, ChatGeneration
from langchain_core.callbacks.manager import CallbackManagerForLLMRun, AsyncCallbackManagerForLLMRun
import asyncio
import aiohttp

class OllamaLLM(LLM):
    """Ollama LLM é€‚é…å™¨ - ä½¿ç”¨åŸç”Ÿ API"""
    
    base_url: str = "http://localhost:11434"
    model: str = "qwen3:1.7b"
    temperature: float = 0.7
    
    @property
    def _llm_type(self) -> str:
        return "ollama"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """è°ƒç”¨ Ollama æ¨¡å‹"""
        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": self.temperature
                    }
                },
                timeout=60
            )
            response.raise_for_status()
            result = response.json()
            return result.get("response", "")
        except Exception as e:
            raise Exception(f"Ollama API è°ƒç”¨å¤±è´¥: {e}")

class OllamaChatModel(BaseChatModel):
    """Ollama èŠå¤©æ¨¡å‹é€‚é…å™¨ - ä½¿ç”¨åŸç”Ÿ API"""

    base_url: str = "http://localhost:11434"

    @property
    def api_url(self) -> str:
        """è·å–æ­£ç¡®çš„ API URL"""
        # ç§»é™¤ /v1 åç¼€ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨åŸç”Ÿ API
        base = self.base_url.rstrip('/v1').rstrip('/')
        return base
    model: str = "qwen3:1.7b"
    temperature: float = 0.7
    streaming: bool = True

    @property
    def model_name(self) -> str:
        """è¿”å›æ¨¡å‹åç§°"""
        return self.model

    @property
    def openai_api_key(self) -> str:
        """è¿”å› API å¯†é’¥ï¼ˆå…¼å®¹æ€§ï¼‰"""
        return "ollama"

    @property
    def openai_api_base(self) -> str:
        """è¿”å› API åŸºç¡€URLï¼ˆå…¼å®¹æ€§ï¼‰"""
        return self.base_url

    def bind_tools(self, tools, **kwargs):
        """ç»‘å®šå·¥å…·åˆ°æ¨¡å‹"""
        # åˆ›å»ºä¸€ä¸ªæ–°çš„å®ä¾‹ï¼Œä¿å­˜å·¥å…·ä¿¡æ¯
        new_instance = self.__class__(
            base_url=self.base_url,
            model=self.model,
            temperature=self.temperature,
            streaming=self.streaming
        )
        new_instance._bound_tools = tools
        return new_instance

    @property
    def bound_tools(self):
        """è·å–ç»‘å®šçš„å·¥å…·"""
        return getattr(self, '_bound_tools', [])
    
    @property
    def _llm_type(self) -> str:
        return "ollama-chat"
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """ç”ŸæˆèŠå¤©å›å¤ - ä½¿ç”¨å¼‚æ­¥æ–¹æ³•çš„åŒæ­¥åŒ…è£…"""
        # ä½¿ç”¨å¼‚æ­¥æ–¹æ³•çš„åŒæ­¥åŒ…è£…
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(self._agenerate(messages, stop, run_manager, **kwargs))
            return result
        finally:
            loop.close()
    
    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """å¼‚æ­¥ç”ŸæˆèŠå¤©å›å¤"""
        prompt = self._messages_to_prompt(messages)

        # å¦‚æœæœ‰ç»‘å®šçš„å·¥å…·ï¼Œæ·»åŠ å·¥å…·ä¿¡æ¯åˆ°æç¤ºä¸­
        if self.bound_tools:
            tools_info = self._format_tools_for_prompt(self.bound_tools)
            prompt = f"{tools_info}\n\n{prompt}"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.api_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "temperature": self.temperature
                        }
                    },
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    response.raise_for_status()
                    result = await response.json()
                    content = result.get("response", "")

                    # åˆ›å»º AI æ¶ˆæ¯ï¼Œæš‚æ—¶ä¸å¤„ç†å·¥å…·è°ƒç”¨
                    message = AIMessage(content=content)
                    generation = ChatGeneration(message=message)
                    return ChatResult(generations=[generation])

        except Exception as e:
            raise Exception(f"Ollama API å¼‚æ­¥è°ƒç”¨å¤±è´¥: {e}")
    
    async def _astream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[AsyncCallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> AsyncIterator[ChatGeneration]:
        """å¼‚æ­¥æµå¼ç”Ÿæˆ"""
        prompt = self._messages_to_prompt(messages)
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    f"{self.api_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": True,
                        "options": {
                            "temperature": self.temperature
                        }
                    },
                    timeout=aiohttp.ClientTimeout(total=60)
                ) as response:
                    response.raise_for_status()
                    
                    async for line in response.content:
                        if line:
                            try:
                                data = json.loads(line.decode('utf-8'))
                                if 'response' in data:
                                    content = data['response']
                                    if content:
                                        message = AIMessage(content=content)
                                        yield ChatGeneration(message=message)
                                        
                                        if run_manager:
                                            await run_manager.on_llm_new_token(content)
                                            
                                if data.get('done', False):
                                    break
                            except json.JSONDecodeError:
                                continue
                                
        except Exception as e:
            raise Exception(f"Ollama API æµå¼è°ƒç”¨å¤±è´¥: {e}")
    
    def _messages_to_prompt(self, messages: List[BaseMessage]) -> str:
        """å°†æ¶ˆæ¯åˆ—è¡¨è½¬æ¢ä¸ºæç¤ºå­—ç¬¦ä¸²"""
        prompt_parts = []
        
        for message in messages:
            if isinstance(message, HumanMessage):
                prompt_parts.append(f"Human: {message.content}")
            elif isinstance(message, AIMessage):
                prompt_parts.append(f"Assistant: {message.content}")
            else:
                prompt_parts.append(f"{message.type}: {message.content}")
        
        prompt_parts.append("Assistant:")
        return "\n".join(prompt_parts)

    def _format_tools_for_prompt(self, tools) -> str:
        """å°†å·¥å…·ä¿¡æ¯æ ¼å¼åŒ–ä¸ºæç¤ºæ–‡æœ¬"""
        if not tools:
            return ""

        tools_text = "ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å·¥å…·æ¥å¸®åŠ©å›ç­”é—®é¢˜ï¼š\n\n"
        for tool in tools:
            tools_text += f"å·¥å…·åç§°: {tool.name}\n"
            tools_text += f"æè¿°: {tool.description}\n"
            if hasattr(tool, 'args_schema') and tool.args_schema:
                tools_text += f"å‚æ•°: {tool.args_schema}\n"
            tools_text += "\n"

        tools_text += "å¦‚æœéœ€è¦ä½¿ç”¨å·¥å…·ï¼Œè¯·åœ¨å›å¤ä¸­æ˜ç¡®è¯´æ˜è¦ä½¿ç”¨å“ªä¸ªå·¥å…·ä»¥åŠç›¸åº”çš„å‚æ•°ã€‚"
        return tools_text

def create_ollama_chat_model(
    model: str = "qwen3:1.7b",
    base_url: str = "http://localhost:11434",
    temperature: float = 0.7,
    streaming: bool = True
) -> OllamaChatModel:
    """åˆ›å»º Ollama èŠå¤©æ¨¡å‹å®ä¾‹"""
    return OllamaChatModel(
        model=model,
        base_url=base_url,
        temperature=temperature,
        streaming=streaming
    )

def test_ollama_adapter():
    """æµ‹è¯• Ollama é€‚é…å™¨"""
    print("ğŸ§ª æµ‹è¯• Ollama é€‚é…å™¨...")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    llm = create_ollama_chat_model()
    
    # æµ‹è¯•æ¶ˆæ¯
    messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±")]
    
    try:
        # åŒæ­¥è°ƒç”¨
        result = llm._generate(messages)
        print(f"âœ… åŒæ­¥è°ƒç”¨æˆåŠŸ: {result.generations[0].message.content[:100]}...")
        return True
    except Exception as e:
        print(f"âŒ åŒæ­¥è°ƒç”¨å¤±è´¥: {e}")
        return False

async def test_ollama_adapter_async():
    """å¼‚æ­¥æµ‹è¯• Ollama é€‚é…å™¨"""
    print("ğŸ§ª å¼‚æ­¥æµ‹è¯• Ollama é€‚é…å™¨...")
    
    # åˆ›å»ºæ¨¡å‹å®ä¾‹
    llm = create_ollama_chat_model()
    
    # æµ‹è¯•æ¶ˆæ¯
    messages = [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±")]
    
    try:
        # å¼‚æ­¥è°ƒç”¨
        result = await llm._agenerate(messages)
        print(f"âœ… å¼‚æ­¥è°ƒç”¨æˆåŠŸ: {result.generations[0].message.content[:100]}...")
        
        # æµå¼è°ƒç”¨
        print("ğŸ”„ æµ‹è¯•æµå¼è°ƒç”¨...")
        stream_content = ""
        async for chunk in llm._astream(messages):
            stream_content += chunk.message.content
            if len(stream_content) > 50:
                break
        print(f"âœ… æµå¼è°ƒç”¨æˆåŠŸ: {stream_content[:100]}...")
        
        return True
    except Exception as e:
        print(f"âŒ å¼‚æ­¥è°ƒç”¨å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # åŒæ­¥æµ‹è¯•
    test_ollama_adapter()
    
    # å¼‚æ­¥æµ‹è¯•
    asyncio.run(test_ollama_adapter_async())
