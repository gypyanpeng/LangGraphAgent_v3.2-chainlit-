#!/usr/bin/env python3
"""
æµ‹è¯• Ollama è¿æ¥å’Œæ¨¡å‹è°ƒç”¨
"""

import asyncio
import json
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm_loader import load_llm_from_config
from langchain_core.messages import HumanMessage

async def test_ollama_connection():
    """æµ‹è¯• Ollama è¿æ¥"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯• Ollama è¿æ¥...")
    
    try:
        # 1. æµ‹è¯•é…ç½®åŠ è½½
        print("\n1ï¸âƒ£ æµ‹è¯•é…ç½®åŠ è½½...")
        llm = load_llm_from_config(provider="ollama")
        print(f"âœ… LLM é…ç½®åŠ è½½æˆåŠŸ")
        print(f"   æ¨¡å‹: {llm.model_name}")
        print(f"   API Key: {llm.openai_api_key}")
        print(f"   Base URL: {llm.openai_api_base}")
        
        # 2. æµ‹è¯•ç®€å•è°ƒç”¨
        print("\n2ï¸âƒ£ æµ‹è¯•ç®€å•æ¨¡å‹è°ƒç”¨...")
        message = HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•å›å¤ä¸€ä¸‹")
        
        try:
            response = await llm.ainvoke([message])
            print(f"âœ… æ¨¡å‹è°ƒç”¨æˆåŠŸ")
            print(f"   å›å¤: {response.content[:100]}...")
        except Exception as e:
            print(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
            
            # è¯¦ç»†é”™è¯¯ä¿¡æ¯
            if hasattr(e, 'response'):
                print(f"   HTTPçŠ¶æ€ç : {e.response.status_code if hasattr(e.response, 'status_code') else 'N/A'}")
                print(f"   å“åº”å†…å®¹: {e.response.text if hasattr(e.response, 'text') else 'N/A'}")
        
        # 3. æµ‹è¯•æµå¼è°ƒç”¨
        print("\n3ï¸âƒ£ æµ‹è¯•æµå¼è°ƒç”¨...")
        try:
            stream_response = ""
            async for chunk in llm.astream([message]):
                stream_response += chunk.content
                if len(stream_response) > 50:  # é™åˆ¶è¾“å‡ºé•¿åº¦
                    break
            print(f"âœ… æµå¼è°ƒç”¨æˆåŠŸ")
            print(f"   æµå¼å›å¤: {stream_response[:100]}...")
        except Exception as e:
            print(f"âŒ æµå¼è°ƒç”¨å¤±è´¥: {e}")
            print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        return False
    
    return True

def test_ollama_api_direct():
    """ç›´æ¥æµ‹è¯• Ollama API"""
    print("\nğŸ”§ ç›´æ¥æµ‹è¯• Ollama API...")
    
    import requests
    
    try:
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        health_response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if health_response.status_code == 200:
            print("âœ… Ollama æœåŠ¡è¿è¡Œæ­£å¸¸")
            models = health_response.json().get("models", [])
            print(f"   å¯ç”¨æ¨¡å‹æ•°é‡: {len(models)}")
            for model in models:
                if "qwen3:1.7b" in model.get("name", ""):
                    print(f"   âœ… æ‰¾åˆ°ç›®æ ‡æ¨¡å‹: {model['name']}")
        else:
            print(f"âŒ Ollama æœåŠ¡å¼‚å¸¸: {health_response.status_code}")
            
        # æµ‹è¯• OpenAI å…¼å®¹æ¥å£
        api_response = requests.post(
            "http://localhost:11434/v1/chat/completions",
            json={
                "model": "qwen3:1.7b",
                "messages": [{"role": "user", "content": "æµ‹è¯•"}],
                "max_tokens": 10
            },
            timeout=10
        )
        
        if api_response.status_code == 200:
            print("âœ… OpenAI å…¼å®¹æ¥å£æ­£å¸¸")
            result = api_response.json()
            print(f"   å“åº”: {result.get('choices', [{}])[0].get('message', {}).get('content', 'N/A')[:50]}...")
        else:
            print(f"âŒ OpenAI å…¼å®¹æ¥å£å¼‚å¸¸: {api_response.status_code}")
            print(f"   é”™è¯¯å†…å®¹: {api_response.text}")
            
    except requests.exceptions.ConnectionError:
        print("âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡")
        print("   è¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ: ollama serve")
    except requests.exceptions.Timeout:
        print("âŒ è¿æ¥è¶…æ—¶")
        print("   Ollama æœåŠ¡å¯èƒ½å“åº”ç¼“æ…¢")
    except Exception as e:
        print(f"âŒ å…¶ä»–é”™è¯¯: {e}")

def check_environment():
    """æ£€æŸ¥ç¯å¢ƒé…ç½®"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒé…ç½®...")
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    config_path = "config/llm_config.json"
    if os.path.exists(config_path):
        print("âœ… é…ç½®æ–‡ä»¶å­˜åœ¨")
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            default_provider = config.get("default_provider")
            print(f"   é»˜è®¤æä¾›å•†: {default_provider}")
            
            if "ollama" in config.get("models", {}):
                ollama_config = config["models"]["ollama"]
                print(f"   Ollama é…ç½®:")
                print(f"     æ¨¡å‹: {ollama_config.get('model_name')}")
                print(f"     URL: {ollama_config.get('base_url')}")
            else:
                print("âŒ æœªæ‰¾åˆ° Ollama é…ç½®")
    else:
        print("âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æ£€æŸ¥ä¾èµ–
    try:
        import langchain_openai
        print("âœ… langchain_openai å·²å®‰è£…")
    except ImportError:
        print("âŒ langchain_openai æœªå®‰è£…")

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Ollama è¿æ¥è¯Šæ–­å·¥å…·")
    print("=" * 50)
    
    # ç¯å¢ƒæ£€æŸ¥
    check_environment()
    
    # ç›´æ¥ API æµ‹è¯•
    test_ollama_api_direct()
    
    # LangChain é›†æˆæµ‹è¯•
    await test_ollama_connection()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ è¯Šæ–­å®Œæˆ")

if __name__ == "__main__":
    asyncio.run(main())
