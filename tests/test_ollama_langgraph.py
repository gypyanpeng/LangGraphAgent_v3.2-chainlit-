#!/usr/bin/env python3
"""
æµ‹è¯• Ollama é€‚é…å™¨åœ¨ LangGraph ä¸­çš„å·¥ä½œæƒ…å†µ
"""

import asyncio
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from llm_loader import load_llm_from_config
from langchain_core.messages import HumanMessage, AIMessage
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.prebuilt import ToolNode

async def test_simple_ollama_langgraph():
    """æµ‹è¯•ç®€å•çš„ Ollama + LangGraph é›†æˆ"""
    print("ğŸ§ª æµ‹è¯• Ollama + LangGraph é›†æˆ...")
    
    try:
        # 1. åŠ è½½ Ollama æ¨¡å‹
        print("\n1ï¸âƒ£ åŠ è½½ Ollama æ¨¡å‹...")
        llm = load_llm_from_config(provider="ollama")
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {llm.model_name}")
        
        # 2. åˆ›å»ºç®€å•çš„ LangGraph å·¥ä½œæµ
        print("\n2ï¸âƒ£ åˆ›å»º LangGraph å·¥ä½œæµ...")
        
        async def call_model(state: MessagesState):
            """è°ƒç”¨æ¨¡å‹"""
            messages = state["messages"]
            response = await llm.ainvoke(messages)
            return {"messages": [response]}
        
        # åˆ›å»ºå·¥ä½œæµ
        workflow = StateGraph(MessagesState)
        workflow.add_node("agent", call_model)
        workflow.add_edge(START, "agent")
        workflow.add_edge("agent", END)
        
        # ç¼–è¯‘å·¥ä½œæµ
        app = workflow.compile()
        print("âœ… LangGraph å·¥ä½œæµåˆ›å»ºæˆåŠŸ")
        
        # 3. æµ‹è¯•å·¥ä½œæµ
        print("\n3ï¸âƒ£ æµ‹è¯•å·¥ä½œæµ...")
        
        # å‡†å¤‡è¾“å…¥
        inputs = {
            "messages": [HumanMessage(content="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹è‡ªå·±")]
        }
        
        # è¿è¡Œå·¥ä½œæµ
        result = await app.ainvoke(inputs)
        
        if result and "messages" in result:
            last_message = result["messages"][-1]
            print(f"âœ… å·¥ä½œæµè¿è¡ŒæˆåŠŸ")
            print(f"   AIå›å¤: {last_message.content[:100]}...")
            return True
        else:
            print("âŒ å·¥ä½œæµè¿”å›ç»“æœå¼‚å¸¸")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_streaming_ollama_langgraph():
    """æµ‹è¯•æµå¼ Ollama + LangGraph"""
    print("\nğŸ”„ æµ‹è¯•æµå¼ Ollama + LangGraph...")
    
    try:
        # åŠ è½½æ¨¡å‹
        llm = load_llm_from_config(provider="ollama")
        
        # åˆ›å»ºæµå¼å·¥ä½œæµ
        async def call_model_stream(state: MessagesState):
            """æµå¼è°ƒç”¨æ¨¡å‹"""
            messages = state["messages"]
            response = await llm.ainvoke(messages)  # å…ˆç”¨éæµå¼æµ‹è¯•
            return {"messages": [response]}
        
        # åˆ›å»ºå·¥ä½œæµ
        workflow = StateGraph(MessagesState)
        workflow.add_node("agent", call_model_stream)
        workflow.add_edge(START, "agent")
        workflow.add_edge("agent", END)
        
        # ç¼–è¯‘å·¥ä½œæµ
        app = workflow.compile()
        
        # å‡†å¤‡è¾“å…¥
        inputs = {
            "messages": [HumanMessage(content="è¯·ç”¨ä¸€å¥è¯ä»‹ç»äººå·¥æ™ºèƒ½")]
        }
        
        # æµå¼è¿è¡Œ
        print("ğŸ”„ å¼€å§‹æµå¼å¤„ç†...")
        async for output in app.astream(inputs):
            for key, value in output.items():
                if "messages" in value:
                    last_message = value["messages"][-1]
                    if isinstance(last_message, AIMessage):
                        print(f"   æµå¼è¾“å‡º: {last_message.content[:50]}...")
                        break
        
        print("âœ… æµå¼å¤„ç†æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ æµå¼æµ‹è¯•å¤±è´¥: {e}")
        return False

async def test_with_persistence():
    """æµ‹è¯•å¸¦æŒä¹…åŒ–çš„ Ollama + LangGraph"""
    print("\nğŸ’¾ æµ‹è¯•å¸¦æŒä¹…åŒ–çš„ Ollama + LangGraph...")
    
    try:
        # å¯¼å…¥æŒä¹…åŒ–ç›¸å…³æ¨¡å—
        from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver
        
        # åŠ è½½æ¨¡å‹
        llm = load_llm_from_config(provider="ollama")
        
        # åˆ›å»ºæ£€æŸ¥ç‚¹å­˜å‚¨å™¨
        checkpointer = AsyncSqliteSaver.from_conn_string("sqlite:///./data/test_ollama_memory.db")
        
        # åˆ›å»ºå·¥ä½œæµ
        async def call_model(state: MessagesState):
            messages = state["messages"]
            response = await llm.ainvoke(messages)
            return {"messages": [response]}
        
        workflow = StateGraph(MessagesState)
        workflow.add_node("agent", call_model)
        workflow.add_edge(START, "agent")
        workflow.add_edge("agent", END)
        
        # ç¼–è¯‘å¸¦æŒä¹…åŒ–çš„å·¥ä½œæµ
        app = workflow.compile(checkpointer=checkpointer)
        print("âœ… å¸¦æŒä¹…åŒ–çš„å·¥ä½œæµåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•ä¼šè¯
        thread_id = "test_ollama_thread"
        config = {"configurable": {"thread_id": thread_id}}
        
        # ç¬¬ä¸€è½®å¯¹è¯
        inputs1 = {
            "messages": [HumanMessage(content="æˆ‘å«å¼ ä¸‰")]
        }
        
        result1 = await app.ainvoke(inputs1, config=config)
        print(f"âœ… ç¬¬ä¸€è½®å¯¹è¯æˆåŠŸ")
        
        # ç¬¬äºŒè½®å¯¹è¯ï¼ˆæµ‹è¯•è®°å¿†ï¼‰
        inputs2 = {
            "messages": [HumanMessage(content="æˆ‘å«ä»€ä¹ˆåå­—ï¼Ÿ")]
        }
        
        result2 = await app.ainvoke(inputs2, config=config)
        print(f"âœ… ç¬¬äºŒè½®å¯¹è¯æˆåŠŸ")
        print(f"   AIå›å¤: {result2['messages'][-1].content[:100]}...")
        
        return True
        
    except Exception as e:
        print(f"âŒ æŒä¹…åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ Ollama + LangGraph é›†æˆæµ‹è¯•")
    print("=" * 50)
    
    # åŸºç¡€æµ‹è¯•
    success1 = await test_simple_ollama_langgraph()
    
    # æµå¼æµ‹è¯•
    success2 = await test_streaming_ollama_langgraph()
    
    # æŒä¹…åŒ–æµ‹è¯•
    success3 = await test_with_persistence()
    
    print("\n" + "=" * 50)
    print("ğŸ¯ æµ‹è¯•ç»“æœæ€»ç»“:")
    print(f"   åŸºç¡€é›†æˆ: {'âœ… æˆåŠŸ' if success1 else 'âŒ å¤±è´¥'}")
    print(f"   æµå¼å¤„ç†: {'âœ… æˆåŠŸ' if success2 else 'âŒ å¤±è´¥'}")
    print(f"   æŒä¹…åŒ–åŠŸèƒ½: {'âœ… æˆåŠŸ' if success3 else 'âŒ å¤±è´¥'}")
    
    if success1 and success2 and success3:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Ollama é€‚é…å™¨å¯ä»¥æ­£å¸¸å·¥ä½œ")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•")

if __name__ == "__main__":
    asyncio.run(main())
